import json
import os 
import time
from elasticsearch import Elasticsearch
from sentence_transformers import SentenceTransformer
import requests

# (í”„ë¡ì‹œ ìš°íšŒ ì„¤ì •ì€ ê·¸ëŒ€ë¡œ ë‘¡ë‹ˆë‹¤)
os.environ['NO_PROXY'] = 'localhost,127.0.0.1'

# --- ì„¤ì • (ì´í•˜ ë™ì¼) ---
ELASTICSEARCH_HOST = "http://127.0.0.1:9200"
INDEX_NAME = "customs-docs-v1"
EMBEDDING_MODEL = 'jhgan/ko-sroberta-multitask' 
OLLAMA_API_URL = "http://localhost:11434/api/chat" 
OLLAMA_MODEL = "llama3:8b" 

# --- âš¡ï¸ [ìˆ˜ì •] AI ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (í•œêµ­ì–´ ì§€ì‹œ ê°•í™”) âš¡ï¸ ---
SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ê´€ì„¸ì²­ì˜ ê³µì‹ AI ì—ì´ì „íŠ¸ 'ì»¤ìŠ¤í…€-ë´‡'ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì˜¤ì§ ì œê³µë˜ëŠ” [ê´€ì„¸ì²­ ê³µì‹ ìë£Œ]ë¥¼ ê·¼ê±°ë¡œ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

[ì§€ì‹œ ì‚¬í•­]
1. ì‚¬ìš©ìì˜ [ì§ˆë¬¸]ì— ë‹µë³€í•˜ê¸° ìœ„í•´, [ê´€ì„¸ì²­ ê³µì‹ ìë£Œ]ì—ì„œë§Œ ê·¼ê±°ë¥¼ ì°¾ìœ¼ì„¸ìš”.
2. ë‹µë³€ì€ ëª…í™•í•˜ê³ , ì´í•´í•˜ê¸° ì‰¬ìš´ í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.
3. ë§Œì•½ [ê´€ì„¸ì²­ ê³µì‹ ìë£Œ]ì— ë‹µë³€ì˜ ê·¼ê±°ê°€ ë˜ëŠ” ë‚´ìš©ì´ ì—†ë‹¤ë©´, "ì£„ì†¡í•©ë‹ˆë‹¤ë§Œ, ì œê³µëœ ìë£Œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
4. ì ˆëŒ€ [ê´€ì„¸ì²­ ê³µì‹ ìë£Œ]ì— ì—†ëŠ” ë‚´ìš©ì„ ì¶”ì¸¡í•˜ê±°ë‚˜ ì„ì˜ì˜ ì •ë³´ë¥¼ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.
5. [ë§¤ìš° ì¤‘ìš”] ëª¨ë“  ë‹µë³€ì€ ë°˜ë“œì‹œ, ë¬´ì¡°ê±´, ì˜ˆì™¸ ì—†ì´ **í•œêµ­ì–´ë¡œë§Œ** ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤. (Do not write in English.)
"""
# ----------------------------------------------------------------

USER_PROMPT_TEMPLATE = """
[ê´€ì„¸ì²­ ê³µì‹ ìë£Œ]
{retrieved_documents}
---
[ì§ˆë¬¸]
{user_query}
"""

class CustomsRAGAgent:
    def __init__(self):
        # (ì´í•˜ __init__ í•¨ìˆ˜ ë‚´ìš©ì€ ë™ì¼)
        print("AI ì—ì´ì „íŠ¸ ì´ˆê¸°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        try:
            print(f"'{EMBEDDING_MODEL}' ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
            self.embedding_model = SentenceTransformer(EMBEDDING_MODEL)
            print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
            print(f"'{ELASTICSEARCH_HOST}'ì— ì—°ê²°ì„ ì‹œë„í•©ë‹ˆë‹¤...")
            self.es_client = Elasticsearch(ELASTICSEARCH_HOST)
            if not self.es_client.ping():
                raise ConnectionError("Elasticsearchì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("âœ… Elasticsearch ì—°ê²° ì„±ê³µ.")
        except Exception as e:
            print(f"âŒ ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("Elasticsearch Docker ì»¨í…Œì´ë„ˆê°€ ì‹¤í–‰ ì¤‘ì¸ì§€, ëª¨ë¸ ì´ë¦„ì´ ì •í™•í•œì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
            exit()

    def retrieve(self, query, top_k=3):
        # (ì´í•˜ retrieve í•¨ìˆ˜ ë‚´ìš©ì€ ë™ì¼)
        print(f"\n[1/3] '{query}' (ì™€)ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤...")
        try:
            query_vector = self.embedding_model.encode(query).tolist()
            knn_query = {
                "field": "content_vector",
                "query_vector": query_vector,
                "k": top_k,
                "num_candidates": 10
            }
            response = self.es_client.search(
                index=INDEX_NAME,
                knn=knn_query,
                source=["source", "content"],
                size=top_k
            )
            hits = response['hits']['hits']
            if not hits:
                print("âš ï¸  ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return ""
            context = ""
            for i, hit in enumerate(hits):
                context += f"\n--- ë¬¸ì„œ {i+1} (ì¶œì²˜: {hit['_source']['source']}) ---\n"
                context += hit['_source']['content']
                context += "\n-----------------------------------\n"
            print(f"âœ… ì´ {len(hits)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            return context
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return ""

    def generate_answer(self, query, context):
        # (ì´í•˜ generate_answer í•¨ìˆ˜ ë‚´ìš©ì€ ë™ì¼)
        print("[2/3] ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤...")
        user_content = USER_PROMPT_TEMPLATE.format(
            retrieved_documents=context,
            user_query=query
        )
        payload = {
            "model": OLLAMA_MODEL,
            "messages": [
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_content}
            ],
            "stream": False 
        }
        proxies_to_use = {
            "http": None,
            "https": None,
        }
        try:
            response = requests.post(
                OLLAMA_API_URL, 
                json=payload, 
                timeout=60,
                proxies=proxies_to_use 
            )
            response.raise_for_status()
            result = response.json()
            answer = result.get("message", {}).get("content", "").strip()
            print("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ.")
            return answer
        except requests.exceptions.ConnectionError:
            print(f"âŒ API í˜¸ì¶œ ì˜¤ë¥˜: Ollama ì„œë²„({OLLAMA_API_URL})ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("Ollamaê°€ ë¡œì»¬ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        except requests.exceptions.HTTPError as e:
            print(f"âŒ API í˜¸ì¶œ ì˜¤ë¥˜: {e}") 
        except requests.exceptions.Timeout:
            print(f"âŒ API í˜¸ì¶œ ì˜¤ë¥˜: ì‘ë‹µ ì‹œê°„(60ì´ˆ)ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ API í˜¸ì¶œ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def ask(self, query):
        # (ì´í•˜ ask í•¨ìˆ˜ ë‚´ìš©ì€ ë™ì¼)
        context_docs = self.retrieve(query)
        if not context_docs:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        answer = self.generate_answer(query, context_docs)
        print("[3/3] ìµœì¢… ë‹µë³€ì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
        return answer

def main():
    # (ì´í•˜ main í•¨ìˆ˜ ë‚´ìš©ì€ ë™ì¼)
    try:
        agent = CustomsRAGAgent()
    except Exception as e:
        return
    print("\n" + "="*50)
    print("ê´€ì„¸ í–‰ì • AI ì—ì´ì „íŠ¸ 'ì»¤ìŠ¤í…€-ë´‡'ì…ë‹ˆë‹¤.")
    print("ê°œì¸ í†µê´€, í•´ì™¸ ì§êµ¬ ë“±ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”.")
    print(f"(LLM: {OLLAMA_MODEL} @ Ollama)")
    print("(ì¢…ë£Œí•˜ì‹œë ¤ë©´ 'q' ë˜ëŠ” 'exit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.)")
    print("="*50)
    while True:
        query = input("\n[ì§ˆë¬¸ ì…ë ¥] > ")
        if query.lower() in ['q', 'exit']:
            print("ì—ì´ì „íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        if not query:
            continue
        start_time = time.time()
        answer = agent.ask(query)
        end_time = time.time()
        print("\n[ì»¤ìŠ¤í…€-ë´‡ ë‹µë³€] ğŸ¤–:")
        print(answer)
        print(f"\n(ë‹µë³€ ìƒì„± ì‹œê°„: {end_time - start_time:.2f}ì´ˆ)")

if __name__ == "__main__":
    main()