import requests
import os

# --- âš¡ï¸ í”„ë¡ì‹œ ìš°íšŒ ì„¤ì • (ëª¨ë“  ë°©ë²• ë™ì›) âš¡ï¸ ---
# 1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ['NO_PROXY'] = 'localhost,127.0.0.1'

# 2. requests ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ëª…ì‹œì ìœ¼ë¡œ í”„ë¡ì‹œ ì—†ìŒì„ ì„ ì–¸
proxies_to_use = {
    "http": None,
    "https": None,
}
# ---------------------------------------------

OLLAMA_API_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL = "llama3:8b"

print(f"Ollama ì„œë²„ì— ì—°ê²°ì„ ì‹œë„í•©ë‹ˆë‹¤: {OLLAMA_API_URL}")
print(f"í”„ë¡ì‹œ ì„¤ì •: {proxies_to_use}")

payload = {
    "model": OLLAMA_MODEL,
    "messages": [
        {"role": "user", "content": "hi"}
    ],
    "stream": False
}

try:
    response = requests.post(
        OLLAMA_API_URL,
        json=payload,
        timeout=10,
        proxies=proxies_to_use # ğŸ‘ˆ í”„ë¡ì‹œ "ì—†ìŒ"ì„ ê°•ì œë¡œ ì§€ì •
    )

    # HTTP ìƒíƒœ ì½”ë“œ í™•ì¸
    print(f"\nHTTP ìƒíƒœ ì½”ë“œ: {response.status_code}")
    response.raise_for_status() # 4xx/5xx ì—ëŸ¬ ì‹œ ì˜ˆì™¸ ë°œìƒ

    # ì„±ê³µ ì‹œ ì‘ë‹µ ì¶œë ¥
    result = response.json()
    print("--- â­ï¸ ì„œë²„ ì‘ë‹µ (ì„±ê³µ) â­ï¸ ---")
    print(result.get("message", {}).get("content", "No content found in message"))

except requests.exceptions.HTTPError as e:
    print("\n--- âŒ [HTTP ì˜¤ë¥˜ ë°œìƒ] âŒ ---")
    print(f"ì˜¤ë¥˜: {e}")
    print("Ollama ì„œë²„ê°€ 4xx ë˜ëŠ” 5xx ì‘ë‹µì„ ë³´ëƒˆìŠµë‹ˆë‹¤.")
    print("ì´ê²ƒì´ 404ë¼ë©´, í”„ë¡ì‹œê°€ ì•„ë‹Œ ë‹¤ë¥¸ ë¬¸ì œì…ë‹ˆë‹¤.")
    print(f"ì „ì²´ ì‘ë‹µ ë‚´ìš©: {response.text}")

except requests.exceptions.ConnectionError as e:
    print("\n--- âŒ [ì—°ê²° ì˜¤ë¥˜ ë°œìƒ] âŒ ---")
    print(f"ì˜¤ë¥˜: {e}")
    print("Ollama ì„œë²„ê°€ êº¼ì ¸ìˆê±°ë‚˜, ë°©í™”ë²½ì— ë§‰í˜€ìˆìŠµë‹ˆë‹¤.")

except Exception as e:
    print(f"\n--- âŒ [ê¸°íƒ€ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜] âŒ ---")
    print(f"ì˜¤ë¥˜: {e}")